\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[french]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[T1]{fontenc}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{color}
\usepackage{fancybox}
\usepackage{biblatex}
\usepackage{lmodern}
\usepackage{tikz}
\usepackage{relsize}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{arabtex}
\usepackage{utf8}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{wrapfig}
\usepackage{listings}
\usepackage{minted}
\usepackage{xcolor}
\algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}%
\usepackage{comment}
\usepackage{lipsum}
\usepackage[ruled,vlined, french, onelanguage]{algorithm2e}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage[left=3cm, right=3cm, top=3.00cm, bottom=3.5cm]{geometry}
\usepackage[Glenn]{fncychap}
\graphicspath{{./figures/}}
\renewcommand{\baselinestretch}{1.2}
\graphicspath{{figures/}}
\usepackage{tocloft}
\renewcommand{\partname}{Partie}

\begin{document}
\begin{center}
\thispagestyle{empty}
\rule{1\textwidth}{0,5pt}\vspace{0,5cm}\\
\textbf{Université de montpellier\\
Faculté des sciences}\\
\vspace{1cm}
\includegraphics[width=4cm, height=4cm]{MONTPELLIER.jpg}
\end{center}

\begin{center}
\textbf{Département mathématique}
\vspace{1cm}
\hspace{2cm}\\
\vspace{1cm}
\textbf{MIND-SIAD}
\vspace{1cm}

\rule{0,8\textwidth}{2pt}\\
\vspace{0,2cm}
\large\textit{\textbf{
TP n°3 : Support Vector Machine (SVM)}} \normalsize
\rule{0,8\textwidth}{2pt}\\ 
 \end{center}
\vspace{1cm}
\begin{center}

ABCHICHE Thiziri \\

01/10/2024  
\end{center}
\newpage

\pagestyle{plain}


\tableofcontents
\newpage




\section{Introduction}
Les machines à vecteurs de support (SVM), introduites par Vapnik, sont 
des méthodes populaires en classification binaire grâce à leur capacité à 
séparer des classes à l'aide d'hyperplans dans des espaces de grande 
dimension. L'objectif de ce TP est d'appliquer les SVM sur des données 
réelles et simulées, tout en explorant l'impact des hyper-paramètres et 
des noyaux au sein du package scikit-learn.\\

Le TP se divise en deux principales parties : la classification sur le 
jeu de données Iris et la classification de visages à partir de la base 
"Labeled Faces in the Wild". Dans la première partie, nous nous
concentrons sur la classification des données Iris en utilisantdes SVM
avec des noyaux linéaires et polynomiaux. La seconde partie porte sur la 
classification de visages, où nous analysons l'influence du paramètre de 
régularisation, l'impact des variables de nuisance, et l'amélioration des 
performances par la réduction de dimension via l'Analyse en Composantes 
Principales (PCA).
\newpage
\section{Définitions et Notations}


Les machines à vecteurs de support (SVM) sont des algorithmes de
classification binaire qui reposent sur la recherche de règles de 
décision linéaires, appelées hyperplans, pour séparer les classes de 
données.

\subsection{Notations}
Dans le cadre de la classification binaire supervisée, nous utilisons les 
notations suivantes :

\begin{itemize}
    \item $Y$ : l'ensemble des étiquettes (ou labels), généralement défini comme $Y = \{-1, 1\}$ pour la classification binaire.
    \item $x = (x_1, \ldots, x_p) \in X \subset \mathbb{R}^p$ : une observation (ou exemple) dans un espace de caractéristiques.
    \item $D_n = \{(x_i, y_i), i = 1, \ldots, n\}$ : un ensemble d'apprentissage contenant $n$ exemples et leurs étiquettes associées.
    \item Il existe un modèle probabiliste qui gouverne la génération de nos observations selon des variables aléatoires $X$ et $Y$ : pour tout $i \in \{1, \ldots, n\}$, $(x_i, y_i) \overset{\text{i.i.d}}{\sim} (X, Y)$.
\end{itemize}

\subsection{Fonction de Décision}
L'objectif est de construire, à partir de l'ensemble d'apprentissage 
$D_n$, une fonction $\hat{f} : X \to \{-1, 1\}$ qui prédit l'étiquette 
pour un point inconnu $x$ (non présent dans l'ensemble d'apprentissage) : 
$\hat{f}(x)$. La règle de décision est dite linéaire, dans le sens où 
elle sépare l'espace par un hyperplan affine.

\subsection{SVM Non Linéaires}
Les SVM non linéaires utilisent une fonction implicite $\Phi$ qui 
transforme l'espace d'entrée $X \subset \mathbb{R}^p$ en un espace
hilbertien $(H, \langle \cdot, \cdot \rangle)$ de dimension supérieure.
L'apprentissage s'effectue alors dans cet espace, où l'on espère que les 
données soient plus linéairement séparables.

\subsection{Noyaux}
La sélection d'un noyau approprié est cruciale pour la performance du
modèle. Parmi les noyaux couramment utilisés, on trouve :

\begin{itemize}
    \item Noyau linéaire : $K(x, x') = \langle x, x' \rangle$.
    \item Noyau Gaussien radial (RBF) : $K(x, x') = \exp(-\gamma \|x - x'\|^2)$.
    \item Noyaux polynomiaux : $K(x, x') = (\alpha + \beta \langle x, x' \rangle)^\delta$ pour $\delta > 0$.
\end{itemize}

\subsection{Formulation du Classificateur SVM}
Un classificateur SVM est généralement formulé comme suit :

\[
\hat{f}_{w, w_0}(x) = \text{sign}(\langle w, \Phi(x) \rangle + w_0),
\]

où $w \in H$ et $w_0 \in \mathbb{R}$ sont des paramètres ajustés lors de l'apprentissage. 

\subsection{Optimisation}
La maximisation de la marge séparant les classes revient à résoudre un 
problème d'optimisation sous contraintes, formulé comme suit :

\[
\begin{aligned}
    (w^*, w_0^*, \xi^* \in \mathbb{R}^n) \in \arg\min_{w \in H, w_0 \in \mathbb{R}, \xi \in \mathbb{R}^n} \left( \frac{1}{2} \|w\|^2 + C \sum_{i=1}^{n} \xi_i \right) \\
    \text{sous } \xi_i \geq 0, \, \forall i \in \{1, \ldots, n\}, \\
    y_i (\langle w, \Phi(x_i) \rangle + w_0) \geq 1 - \xi_i, \, \forall i \in \{1, \ldots, n\}.
\end{aligned}
\]

Le paramètre $C$ contrôle la complexité du classificateur, déterminant le 
coût des erreurs de classification.
\newpage


\section{Application des SVM pour la classification des iris}
\subsection{Introduction au jeu de données Iris}
Dans cette section, nous introduirons le jeu de données Iris, qui est un
ensemble de données largement utilisé pour tester les algorithmes de 
classification. Il contient 150 échantillons de fleurs d'iris, répartis 
en trois espèces (Iris setosa, Iris versicolor et Iris virginica), avec 
quatre caractéristiques mesurées pour chaque échantillon : la longueur et 
la largeur des sépales, ainsi que la longueur et la largeur des pétales.
\subsection{Chargement des bibliothèques nécessaires}

Pour réaliser notre classification, nous utiliserons les bibliothèques suivantes :

\begin{itemize}
    \item \textbf{NumPy} : pour la manipulation des tableaux et des calculs numériques.
    \item \textbf{Matplotlib} : pour la visualisation des données.
    \item \textbf{scikit-learn} : pour l'implémentation des SVM et le traitement du jeu de données.
\end{itemize}

Voici le code pour importer ces bibliothèques :

\begin{minted}[frame=single, bgcolor=lightgray, rulecolor=black, fontsize=\footnotesize]{python}
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
\end{minted}

\subsection{Chargement du jeu de données Iris}
Nous allons charger le jeu de données Iris à partir de scikit-learn. Ce 
processus inclut l'accès aux données et leur séparation en 
caractéristiques (features) et étiquettes (labels).
\begin{minted}[frame=single, bgcolor=lightgray, rulecolor=black, fontsize=\footnotesize]{python}
# Chargement des données Iris et prétraitement
iris = datasets.load_iris()
X = iris.data
scaler = StandardScaler()  # Normalisation des données
X = scaler.fit_transform(X)
y = iris.target
\end{minted}


Par le suite, nous allons garder uniquement deux classes pour effectuer
une classification binaire. Pour ce faire, nous allons :\\
\begin{itemize}
\item Sélectionner les classes : Nous allons conserver uniquement les 
exemples appartenant aux classes 1 et 2 (Iris versicolor et Iris
virginica), ce qui nous permettra d'effectuer une classification 
binaire.\\
\item Réduire les caractéristiques : Pour faciliter la visualisation,
nous allons ne conserver que les deux premières caractéristiques 
(longueur et largeur des sépales) du jeu de données.\\
\item Mélanger et diviser les données : Ensuite, nous allons mélanger les 
données pour garantir une répartition aléatoire des exemples, puis les 
diviser en ensembles d'entraînement et de test, avec une proportion de 
50\% pour chaque ensemble.\\
\end{itemize}

Voici le code correspondant :

\begin{minted}[frame=single, bgcolor=lightgray, rulecolor=black, fontsize=\footnotesize]{python}
# On garde uniquement deux classes pour avoir une classification binaire (classe 1 et 2)
X = X[y != 0, :2]  # Prendre seulement 2 caractéristiques pour visualisation
y = y[y != 0]

# Mélanger les données et diviser en train/test
X, y = shuffle(X, y, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)
\end{minted}
\subsection{Noyau linéaire}

Dans cette étape, nous allons utiliser un noyau linéaire pour le
classificateur SVM et effectuer une recherche des meilleurs paramètres de 
régularisation à l'aide de la validation croisée. Nous allons explorer 
différentes valeurs pour le paramètre $C$, qui contrôle la complexité du 
modèle. Un $C$ faible peut entraîner un modèle trop simple, tandis qu'un 
$C$ élevé peut mener à un modèle plus complexe qui pourrait surajuster 
les données.

Voici le code correspondant :

\begin{minted}[frame=single, bgcolor=lightgray, rulecolor=black, fontsize=\footnotesize]{python}
# Noyau linéaire
# GridSearch pour le noyau linéaire avec différents paramètres de régularisation (C)
parameters = {'kernel': ['linear'], 'C': list(np.logspace(-3, 3, 200))}
# Recherche des meilleurs paramètres avec validation croisée
clf_linear = GridSearchCV(SVC(), parameters, cv=5)  
clf_linear.fit(X_train, y_train)  # Entraînement du modèle

# Calcul du score (précision) sur les ensembles d'entraînement et de test
print('Meilleurs paramètres pour noyau linéaire:', clf_linear.best_params_)
print('Score de généralisation pour noyau linéaire: train = %.2f, test = %.2f' %
      (clf_linear.score(X_train, y_train), clf_linear.score(X_test, y_test)))
\end{minted}

\begin{itemize}
    \item \textbf{Meilleurs paramètres} : \{ 'C': \texttt{np.float64(0.29673024081888694)}, 'kernel': 'linear' \}
    \item \textbf{Score de généralisation} :
    \begin{itemize}
        \item Entraînement : $0.66$
        \item Test : $0.66$
    \end{itemize}
\end{itemize}

Ces résultats indiquent que le modèle présente une performance de 66\% 
tant sur l'ensemble d'entraînement que sur l'ensemble de test, suggérant 
qu'il est capable de généraliser les données sans surajuster.

 \section{Comparaiosn des modèles SVM avec noyaux linéaire et polynomial}

\subsection{Noyau polynomial}

Dans cette étape, nous allons utiliser un noyau polynomial pour le 
classificateur SVM. Nous effectuerons une recherche des meilleurs 
paramètres à l'aide de la validation croisée, en explorant différentes 
valeurs pour les paramètres $C$, $\gamma$, et le degré du polynôme.

Voici le code correspondant :

\begin{minted}[frame=single, bgcolor=lightgray, rulecolor=black, fontsize=\footnotesize]{python}
# Noyau polynomial
# GridSearch pour le noyau polynomial avec paramètres C, gamma, et degree
Cs = list(np.logspace(-3, 3, 5))
gammas = 10. ** np.arange(1, 2)
degrees = np.r_[2, 3]

parameters = {'kernel': ['poly'], 'C': Cs, 'gamma': gammas, 'degree': degrees}
# Recherche des meilleurs paramètres pour noyau polynomial
clf_poly = GridSearchCV(SVC(), parameters, cv=5)  
clf_poly.fit(X_train, y_train)  # Entraînement du modèle

# Affichage des meilleurs paramètres et des scores
print('Meilleurs paramètres pour noyau polynomial:', clf_poly.best_params_)
print('Score de généralisation pour noyau polynomial: train = %.2f, test = %.2f' %
      (clf_poly.score(X_train, y_train), clf_poly.score(X_test, y_test)))
\end{minted}


Après avoir effectué la recherche des meilleurs paramètres, nous avons
obtenu les résultats suivants :

\begin{itemize}
    \item \textbf{Meilleurs paramètres} : \{ 'C': \texttt{np.float64(0.001)}, 'degree': \texttt{np.int64(2)}, 'gamma': \texttt{np.float64(10.0)}, 'kernel': 'poly' \}
    \item \textbf{Score de généralisation} :
    \begin{itemize}
        \item Entraînement : $0.64$
        \item Test : $0.44$
    \end{itemize}

\subsection{\textbf{Comparaison:}}

\begin{figure}[H]
    \centering
    \begin{minipage}{0.9\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure1.png}
        \caption{Frontière de décision}
    \end{minipage}
    \end{figure}
Cette figure ci-dessus, représente un SVM avec un noyau polynomial de 
degré 
2, montre une séparation quadratique des données avec les paramètres 
optimaux 
: {'C': 0,001, 'degree': 2, 'gamma': 10.0, 'kernel': 'poly'}. Bien que ce 
modèle puisse modéliser des frontières de décision plus complexes, il 
présente une performance inférieure, avec un score de généralisation de
0,64 
sur l'entraînement et 0,44 sur le test.
 \begin{figure}[H]
    \centering
    \begin{minipage}{0.9\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure2.png}
        \caption{Frontière de décision}
    \end{minipage}
    \end{figure}

En revanche, la figure ci-dessus, représente un SVM avec un noyau
polynomial 
de degré 1. Le faible paramètre \( C = 0,001 \) indique une grande marge 
de 
séparation, entraînant un underfitting, où le modèle ne capture pas 
suffisamment d'informations à partir des données d'entraînement. En 
revanche, 
le modèle avec un noyau linéaire (degré 1) et \( C = 0,031 \) obtient de 
meilleurs scores de \( 0,66 \) sur les deux ensembles. Cela suggère 
qu'une 
séparation linéaire est plus appropriée pour ce problème.

\section{Analyse de l'impact de la régularisation sur le modèle SVM avec
noyau linéaire}

Dans cette section, nous allons examiner l'impact de la diminution du 
paramètre de régularisation C sur le modèle SVM avec un noyau 
linéaire.Avant d'aborder l'impact du paramètre \( C \), nous allons 
d'abord lancer le script \texttt{svm\_gui.py}, qui permet de visualiser 
et manipuler en temps réel les décisions d'un SVM. Ce script propose une 
interface graphique où l'on peut créer des points de données et observer 
comment le modèle SVM sépare les classes.\\

Pour générer un jeu de données déséquilibré avec une répartition de \( 
90\% \) de points dans une classe et \( 10\% \) dans l'autre, suivez les
étapes suivantes :\\

\begin{itemize}
    \item \textbf{Création des points rouges (classe positive)} : Cliquez sur l'interface graphique avec le bouton gauche de la souris pour ajouter des points rouges représentant la classe positive.
    \item \textbf{Création des points noirs (classe négative)} : Utilisez le bouton droit de la souris pour ajouter des points noirs représentant la classe négative.
\end{itemize}

En générant un jeu de données très déséquilibré, nous allons pouvoir 
observer comment le modèle SVM se comporte en présence de classes 
déséquilibrées.


\
\begin{figure}[H]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{capture2.PNG}
        \caption{Avec un C élevé}
    \end{minipage}\hfill
    \begin{minipage}{0.46\textwidth}
        \centering
        \includegraphics[width=\textwidth]{capture3.PNG}
        \caption{Avec un C faible }
    \end{minipage}
\end{figure}
Lorsque le paramètre de régularisation \( C \) est élevé (par exemple 
avec \( C = 10000 \)), le SVM impose une séparation stricte entre les 
classes, minimisant les erreurs de classification mais avec une marge 
très étroite. En réduisant \( C \) (par exemple à \( C = 0.001 \)), le 
modèle devient plus souple, élargissant la marge au prix de quelques 
erreurs, surtout pour les points proches de la frontière de décision. 
Avec un \( C \) encore plus faible (par exemple \( C = 0.00001 \)), le 
SVM priorise l'élargissement maximal de la marge, mais cela entraîne une 
classification moins précise, particulièrement pour la classe 
minoritaire. Cela montre qu'un \( C \) trop faible favorise une 
généralisation excessive, avec une perte de précision au profit d'une 
marge plus large.\\
\section{Introduction à la classification de visages}

Dans cette section, nous allons travailler sur un problème de 
classification de visages en utilisant le jeu de données Labeled Faces in 
the Wild (LFW). Ce jeu de données est accessible à l'adresse suivante : 
\texttt{http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz}.\\
\subsection{Chargement des données}

Nous allons charger le jeu de données et extraire les caractéristiques 
pertinentes pour la classification. Voici un exemple de code pour charger 
les données et préparer les images 
\begin{minted}[frame=single, bgcolor=lightgray, rulecolor=black, fontsize=\footnotesize]{python}
from sklearn.datasets import fetch_lfw_people

# Chargement des données 
lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4,
                              color=True, funneled=False, slice_=None,
                              download_if_missing=True)
\end{minted}

\subsection{Examen des dimensions des images}

Après avoir chargé les données, nous examinons les tableaux d'images pour 
en déterminer leurs dimensions. Cela nous permettra de mieux comprendre 
la structure des données.


Voici le code pour cette étape :

\begin{minted}[frame=single, bgcolor=lightgray, rulecolor=black, fontsize=\footnotesize]{python}
# Examinez les tableaux d'images pour en déterminer les dimensions 
images = lfw_people.images
n_samples, h, w, n_colors = images.shape
\end{minted}
\subsection{Identification des étiquettes}

L'étiquette à prédire est l'identifiant de la personne. Nous allons 
extraire les noms des cibles à partir des données.

Voici le code :

\begin{minted}[frame=single, bgcolor=lightgray, rulecolor=black, fontsize=\footnotesize]{python}
# L'étiquette à prédire est l'identifiant de la personne
target_names = lfw_people.target_names.tolist()
\end{minted}

\subsection{Sélection des paires à classifier}

Pour notre classification, nous choisissons une paire de personnes à 
classifier, par exemple, \textit{Tony Blair} et \textit{Colin Powell}. 

Voici le code correspondant :

\begin{minted}[frame=single, bgcolor=lightgray, rulecolor=black, fontsize=\footnotesize]{python}
# Choisissez une paire à classifier, par exemple
names = ['Tony Blair', 'Colin Powell']
# names = ['Donald Rumsfeld', 'Colin Powell']
\end{minted}
\subsection{Préparation des images pour la classification}

Nous allons maintenant préparer les images correspondant aux deux 
personnes sélectionnées pour effectuer la classification. Nous 
construisons également le tableau des étiquettes.

Voici le code :

\begin{minted}[frame=single, bgcolor=lightgray, rulecolor=black, fontsize=\footnotesize]{python}
idx0 = (lfw_people.target == target_names.index(names[0]))
idx1 = (lfw_people.target == target_names.index(names[1]))
images = np.r_[images[idx0], images[idx1]]
n_samples = images.shape[0]
y = np.r_[np.zeros(np.sum(idx0)), np.ones(np.sum(idx1))].astype(int)

# Tracez un ensemble d'échantillons des données
plot_gallery(images, np.arange(12))
plt.show()
\end{minted}

la figure suivante montre donc l'échantillon des données pour visualiser 
les images de \textit{Tony Blair} et \textit{Colin Powell}

\begin{figure}[H]
    \centering
    \begin{minipage}{0.9\textwidth}
        \centering
        \includegraphics[width=\textwidth]{vsg1.png}
        \caption{classification des visages}
    \end{minipage}
    \end{figure}

\subsection{Extraction des caractéristiques}

Nous allons extraire les caractéristiques à partir des images de visages.
Pour ce faire, nous utiliserons uniquement les illuminations, ce qui nous 
permettra de simplifier le modèle. Voici le code correspondant :

\begin{minted}[frame=single, bgcolor=lightgray, rulecolor=black, fontsize=\footnotesize]{python}
# Extraire les caractéristiques
# Caractéristiques utilisant uniquement les illuminations.
X = (np.mean(images, axis=3)).reshape(n_samples, -1)

# Optionnel : Ou calculez les caractéristiques 
#en utilisant les couleurs (3 fois plus de caractéristiques)
# X = images.copy().reshape(n_samples, -1)
\end{minted}
Avant de procéder à l'entraînement, nous normalisons les caractéristiques
pour qu'elles aient une moyenne nulle et un écart-type de un. Cela aide 
le modèle à converger plus rapidement.

Voici le code :

\begin{minted}[frame=single, bgcolor=lightgray, rulecolor=black, fontsize=\footnotesize]{python}
# Scale features
X -= np.mean(X, axis=0)
X /= np.std(X, axis=0)
\end{minted}


Nous allons ensuite diviser les données en ensembles d'entraînement et de 
test, en utilisant 50\% des données pour chaque ensemble. Cela nous 
permettra d'évaluer la performance du modèle,comme le montre le code
suivant:

\begin{minted}[frame=single, bgcolor=lightgray, rulecolor=black, fontsize=\footnotesize]{python}
# Split data into a half training and half test set
indices = np.random.permutation(X.shape[0])
train_idx, test_idx = indices[:X.shape[0] // 2], indices[X.shape[0] // 2:]
X_train, X_test = X[train_idx, :], X[test_idx, :]
y_train, y_test = y[train_idx], y[test_idx]
images_train, images_test = images[train_idx, :, :, :], images[test_idx, :, :, :]
\end{minted}
Nous allons maintenant entraîner un modèle SVM avec un noyau linéaire en 
testant différentes valeurs du paramètre de régularisation \( C \) et 
observer comment cela influence les performances. 

Voici le code :

\begin{minted}[frame=single, bgcolor=lightgray, rulecolor=black, fontsize=\footnotesize]{python}
# L'étiquette à prédire est l'identifiant de la personne 
print("--- Linear kernel ---")
print("Fitting the classifier to the training set")
t0 = time()

# Fit a classifier (linear) and test all the Cs
Cs = 10. ** np.arange(-5, 6)
scores = []

# Boucle sur les valeurs de C
for C in Cs:
    # Créer un classificateur SVM avec le noyau linéaire et le paramètre C
    clf = svm.SVC(kernel='linear', C=C)
    clf.fit(X_train, y_train)  # Entraîner le modèle sur les données d'entraînement

    # Prédire les labels pour l'ensemble de test
    y_pred = clf.predict(X_test)

    # Calculer le score (précision) et l'ajouter à la liste des scores
    score = np.mean(y_pred == y_test)
    scores.append(score)

# Trouver l'indice du meilleur score
ind = np.argmax(scores)
print("Best C: {}".format(Cs[ind]))
\end{minted}
\begin{itemize}
    \item \textbf{Meilleur paramètre \( C \)} : \( 1.0 \)
    \item \textbf{Scores de généralisation} :
    \begin{itemize}
        \item Entraînement : \text{À insérer}
        \item Test : \text{À insérer}
    \end{itemize}
\end{itemize}
Afficher ensuite l'erreur de prédiction en fonction du paramètre de 
régularisation \( C \) sur une échelle logarithmique.

\begin{minted}[frame=single, bgcolor=lightgray, rulecolor=black, fontsize=\footnotesize]{python}
plt.figure()
plt.plot(Cs, scores)
plt.xlabel("Paramètre de régularisation C")
plt.ylabel("Scores de prédiction")
plt.xscale("log")
plt.tight_layout()
plt.show()
print("Best score: {}".format(np.max(scores)))
\end{minted}
--- Linear kernel ---\\
Fitting the classifier to the training set\\
Best C: 0.001\\
Best score: 0.9210526315789473\\
Predicting the people names on the testing set\\

\begin{figure}[H]
    \centering
    \begin{minipage}{0.9\textwidth}
        \centering
        \includegraphics[width=\textwidth]{grph1.png}
        \caption{Régularisation du paramètre C}
    \end{minipage}
    \end{figure}
Ce graphe montre que la meilleure régularisation est obtenue avec \( C =
0.001 \), où le modèle atteint une précision de 90\%. Pour des valeurs 
très faibles de \( C \), la précision est d'environ 65\%, mais elle 
augmente rapidement jusqu'à \( C = 0.001 \), avant de se stabiliser. Cela 
indique que cette valeur de \( C \) offre un bon équilibre entre la 
généralisation et la performance pour cette tâche de reconnaissance de 
visages.


\subsection{Évaluation finale}

Nous allons prédire les étiquettes pour les images de test avec le
meilleur classificateur trouvé et afficher la précision.


Voici le code :

\begin{minted}[frame=single, bgcolor=lightgray, rulecolor=black, fontsize=\footnotesize]{python}
print("Predicting the people names on the testing set")
t0 = time()

# Créez un classificateur SVM avec le meilleur paramètre C
best_C = Cs[ind]
clf = svm.SVC(kernel='linear', C=best_C)
clf.fit(X_train, y_train)  # Entraîner le modèle sur les données d'entraînement

print("done in %0.3fs" % (time() - t0))
# Afficher la précision du modèle
print("Chance level : %s" % max(np.mean(y), 1. - np.mean(y)))
print("Accuracy : %s" % clf.score(X_test, y_test))
\end{minted}
done in 1185.726s\\
Chance level : 0.6210526315789474\\
Accuracy : 0.7\\
La création du modèle SVM avec le paramètre optimal \(C\) a permis 
d'obtenir une précision de 70\% sur l'ensemble de test. Ce score est 
supérieur au "chance level" de 62.1\%, qui correspond à la précision 
obtenue si le modèle prédisait uniquement la classe majoritaire. Le temps 
d'entraînement du modèle a été relativement long (1185.726 secondes), 
probablement en raison de la complexité des données. Ces résultats 
montrent que le modèle SVM est capable de généraliser correctement, bien 
que des améliorations soient encore possibles.

\subsection{Évaluation qualitative des prédictions}

Nous évaluons la qualité des prédictions du modèle en comparant les 
étiquettes prédites (\texttt{y\_pred}) aux étiquettes réelles 
(\texttt{y\_test}). Les images des visages sont affichées avec leurs 
prédictions correspondantes.

Voici le code utilisé pour cette étape :

\begin{minted}[frame=single, bgcolor=lightgray, rulecolor=black, fontsize=\footnotesize]{python}
# Qualitative evaluation of the predictions using matplotlib
prediction_titles = [title(y_pred[i], y_test[i], names)
                     for i in range(y_pred.shape[0])]

# Afficher la galerie d'images avec les prédictions
plot_gallery(images_test, prediction_titles)
plt.show()

# jeter un oeil sur le coefficients 
plt.figure()
plt.imshow(np.reshape(clf.coef_, (h, w)))
plt.show()

\end{minted}

\begin{figure}[H]
    \centering
    \begin{minipage}{0.9\textwidth}
        \centering
        \includegraphics[width=\textwidth]{vsg3.png}
        \caption{Classification de visages}
    \end{minipage}
    \end{figure} 
Cette série d'images compare les prédictions du modèle avec les vraies
étiquettes.

\textbf{Images avec "predicted: Powell, true: Powell"} : Le modèle 
identifie correctement Powell, indiquant une bonne reconnaissance.

\textbf{Images avec "predicted: Blair, true: Blair"} : La prédiction 
correcte pour Blair démontre également la performance du modèle.

\textbf{Cas d'erreurs} : Certaines images montrent des erreurs, comme 
"Blair" prédit pour "Powell". Ces erreurs peuvent être dues à des 
similitudes entre visages ou à une mauvaise généralisation du modèle.
Pour au mieux comprendre cet enjeu, nous allons observer la figure 
suivante:
  
\begin{figure}[H]
    \centering
    \begin{minipage}{0.9\textwidth}
        \centering
        \includegraphics[width=\textwidth]{vsg2.png}
        \caption{Classification de visages}
    \end{minipage}
    \end{figure}
    
Cette image montre les coefficients du modèle SVM appris sur le jeu de
données, visualisés comme une matrice représentant une image. Chaque 
pixel correspond à l'importance d'une caractéristique (ici, un pixel du 
visage) dans la décision de classification du modèle.

Zones lumineuses (jaune/vert clair) : elles indiquent les régions de 
l'image qui ont une forte influence sur la prédiction. Ici, les zones 
autour des yeux, du nez et de la bouche semblent être particulièrement 
importantes.\\

Zones sombres (bleu/noir) : elles correspondent aux parties de l'image 
qui ont peu ou pas d'impact sur la décision du modèle.
Le modèle semble se concentrer sur des zones clés du visage pour 
différencier les classes (ex. Powell vs. Blair), ce qui est cohérent avec 
les parties distinctives du visage humain pour la reconnaissance.

\section{Évaluation du modèle SVM}

Dans cette section, nous allons évaluer les performances d'un modèle SVM.
Nous effectuerons d'abord l'évaluation sur un ensemble de données sans 
variables de nuisance, puis nous ajouterons des variables de nuisance et 
examinerons les performances du modèle.\\


Nous commençons par définir une fonction \texttt{run\_svm\_cv} qui 
prendra en entrée les données d'entrée \(_X\) et les étiquettes \(_y\). 
Cette fonction va diviser les données en ensembles d'entraînement et de 
test, puis effectuer une recherche de grille pour trouver le meilleur 
paramètre de régularisation \(C\).

\begin{minted}[frame=single, bgcolor=lightgray, rulecolor=black, fontsize=\footnotesize]{python}
def run_svm_cv(_X, _y):
    _indices = np.random.permutation(_X.shape[0])
    _train_idx, _test_idx = _indices[:_X.shape[0] // 2], _indices[_X.shape[0] // 2:]
    _X_train, _X_test = _X[_train_idx, :], _X[_test_idx, :]

    _y_train, _y_test = _y[_train_idx], _y[_test_idx]

    _parameters = {'kernel': ['linear'], 'C': list(np.logspace(-3, 3, 5))}
    _svr = svm.SVC()
    _clf_linear = GridSearchCV(_svr, _parameters)
    _clf_linear.fit(_X_train, _y_train)

    print('Generalization score for linear kernel: %s, %s \n' %
          (_clf_linear.score(_X_train, _y_train), _clf_linear.score(_X_test, _y_test)))
\end{minted}

\subsection{Évaluation du modèle sans variable de nuisance}

Nous évaluons d'abord le modèle sans ajouter de bruit aux données. Cela 
permet de voir comment le modèle se comporte sur un ensemble de données 
propre.

\begin{minted}[frame=single, bgcolor=lightgray, rulecolor=black, fontsize=\footnotesize]{python}
print("Score sans variable de nuisance")
run_svm_cv(X, y)  # Exécute la fonction avec les données sans bruit
\end{minted}

% Ajoutez ici le résultat de l'évaluation sans variable de nuisance

\subsection{Évaluation du modèle avec variables de nuisance}

Nous ajoutons ensuite des variables de nuisance. Cela implique de générer
un ensemble de données avec du bruit pour voir comment le modèle gère les 
données moins propres.

\begin{minted}[frame=single, bgcolor=lightgray, rulecolor=black, fontsize=\footnotesize]{python}
print("Score avec variable de nuisance")
n_features = X.shape[1]
# On ajoute des variables de nuisance (bruit)
sigma = 1
noise = sigma * np.random.randn(n_samples, 300)  # Ajout de 300 features bruitées
X_noisy = np.concatenate((X, noise), axis=1)
X_noisy = X_noisy[np.random.permutation(X.shape[0])]

# Exécute la fonction avec les données bruitées
run_svm_cv(X_noisy, y)
\end{minted}

L'ajout de variables de nuisance a un effet notable sur la performance du
modèle. Les résultats montrent que sans ajout de bruit, le modèle atteint 
un score de généralisation parfait sur l'ensemble d'entraînement (1.0) et 
un score de 0.8789 sur l'ensemble de test.

Cependant, après l'ajout de 300 variables de nuisance, bien que le score 
sur l'ensemble d'entraînement reste élevé (0.9947), le score sur 
l'ensemble de test chute drastiquement à 0.5684. Cela indique que l'ajout 
de variables inutiles perturbe la capacité du modèle à généraliser 
correctement, ce qui entraîne une dégradation de ses performances sur des 
données non vues.

\begin{verbatim}
Score sans variable de nuisance:
Entraînement: 1.0, Test: 0.8789

Score avec variable de nuisance:
Entraînement: 0.9947, Test: 0.5684
\end{verbatim}

Cette chute de performance montre clairement que le modèle SVM est 
affecté par l'ajout de bruit, car ces variables de nuisance complexifient 
inutilement la tâche de classification.

\section{Amélioration de la prédiction avec réduction de dimension (PCA)}

Pour améliorer la prédiction, nous avons utilisé une réduction de
dimension à l'aide de l'objet \texttt{PCA} avec le solveur 

\texttt{svd\_solver='randomized'}. Cela permet de conserver les 
informations essentielles tout en réduisant le nombre de 
caractéristiques, notamment après l'ajout de bruit (variables de
nuisance).


\begin{minted}[frame=single, bgcolor=lightgray, rulecolor=black, fontsize=\footnotesize]{python}
print("Score après réduction de dimension")

n_components = 100 
pca = PCA(n_components=n_components).fit(X_noisy)

# Transformation des données bruitées avec PCA
X_noisy_pca = pca.transform(X_noisy)

# Affichage de la variance expliquée pour s'assurer de la qualité de la réduction
print(f"Variance expliquée avec {n_components} composantes principales :

{np.sum(pca.explained_variance_ratio_):.2f}")

# Application du modèle SVM sur les données réduites
run_svm_cv(X_noisy_pca, y)  # Utilisation du modèle SVM sur les données réduites par PCA
\end{minted}

Le résultat obtenu après réduction de dimension montre que la variance 
expliquée avec les 20 premières composantes principales est de 0.68. Cela 
indique que 68\% de la variance totale est conservée dans les données 
après réduction de dimension.

L'application de l'analyse en composantes principales (ACP) a permis de 
réduire la dimensionnalité des données après l'ajout de variables de 
nuisance. Avec 20 composantes principales, 68\%  de la variance totale 
des données a été capturée. Cela signifie que la majorité de 
l'information a été conservée tout en éliminant une partie du bruit 
inutile.


Lorsque le nombre de composantes est inférieur à 20, une plus faible 
proportion de la variance est expliquée, ce qui entraîne une perte 
d'information et potentiellement une dégradation des performances du 
modèle. En revanche, en augmentant le nombre de composantes, la quantité 
d'information capturée augmente, mais avec une complexité plus élevée.
\newpage
\section{Conclusion}
Dans ce TP, nous avons appliqué les machines à vecteurs de support (SVM) 
sur différentes tâches de classification, en comparant l'effet de 
différents noyaux et paramètres de régularisation. Le noyau linéaire a 
montré de bonnes performances sur le jeu de données Iris, surpassant le 
noyau polynomial en termes de généralisation.

Nous avons ensuite étudié l'impact des variables de nuisance, qui ont 
significativement dégradé les performances du modèle. Enfin, 
l'application de la réduction de dimension via PCA a permis d'améliorer 
les performances après l'ajout de bruit, en conservant une proportion 
importante de la variance des données.

Ces expériences ont permis d'approfondir la compréhension des SVM et des
techniques de réduction de dimension.





\end{document}
